<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
<title>Help Wanted</title>
<description>Other</description>
<link>https://lbonanomi.github.io/rss/feed.xml</link>
<item>
	<title>Add Carthage compatibility badge to the README</title>
	<link>https://github.com/github/Archimedes/issues/38</link>
<description>Using [these instructions](https://github.com/Carthage/Carthage/blob/7a0153cc164e301c46527f6e20c886728a0dc218/README.md#declare-your-compatibility).\n"</description>
</item>
	</channel>
	
<channel>
<title>Help Wanted</title>
<description>Help Wanted Issues</description>	
<item>
	<title>Test that license texts match SPDX plain license texts</title>
	<link>https://github.com/github/choosealicense.com/issues/636</link>
<description>We should have a test that each license text in `</description>
</item>
<item>
	<title>Annotating license texts with license rules</title>
	<link>https://github.com/github/choosealicense.com/issues/441</link>
<description>[Comment/question today](https://github.com/github/choosealicense.com/pull/320#issuecomment-230746990) about whether EUPL-1.1 is accurately described reminded me to file this enhancement idea.\n\nFor each license, license rules could be annotated with ranges of text in the license pertinent to the rule. Highlighting of ranges could be turned on/off on individual license pages by selecting in the license rules (permissions/conditions/limitations) table. Very crude mockup taking a very simple case (the one condition of MIT).\n\n![mit-highlight-condition](https://cloud.githubusercontent.com/assets/40415/16633043/25735510-437c-11e6-84f8-1e504d48f345.png)\n\nObviously this is not a big help for MIT, but for longer licenses, it can be tricky to figure out what bits of the license are pertinent for a particular rule, at least if you only want to read once, which is more already than I suspect most people do.\n\nSuch annotations </description>
</item>
<item>
	<title>Add Free Art License</title>
	<link>https://github.com/github/choosealicense.com/issues/314</link>
<description>[Free Art License 1.3](http://artlibre.org/licence/lal/en/)\n"</description>
</item>
<item>
	<title>I18N</title>
	<link>https://github.com/github/choosealicense.com/issues/68</link>
<description>Would love to see about baking in I18N support to choosealicense.com proper. See #67 and #62\n\nWe already have the bulk of the strings in a single file (`</description>
</item>
<item>
	<title>TODO: Configure caching for fast building on travisci</title>
	<link>https://github.com/github/government.github.com/issues/750</link>
<description>Description and details here: https://github.com/gjtorikian/html-proofer#configuring-caching and https://github.com/gjtorikian/html-proofer#caching-with-travis"</description>
</item>
<item>
	<title>XML report for CI</title>
	<link>https://github.com/github/licensed/issues/52</link>
<description>Hi,\r\n\r\nI'm integrating licensed for a poc in our Jenkins pipeline. It could be great to get a report of all licenses checks in an XML formatted file; so we can process it using Jenkins plugins. For now, I have to analyse the `XXX dependencies checked, XXX warnings found.` string formatted output of `licensed status` command.\r\n\r\nWe can consider **licensed results** as **tests results**.\r\n* If all licenses check passed; then the overall test is **passed**.\r\n* If one license is found as unknown, or not matching any of allowed, ignored or reviewed configs; then the check/test **failed**.\r\n\r\nStandards test output formats such as junit, nunit, mstest, google-test, etc. would be great cos already known by many CI tools. Junit is probably the most used one.\r\n\r\nOrganizing tests in \"groups\" matching the package type (npm, pip, go, etc.) and license type (mit, gpl...) would also allows to \"count\" the type of licenses found... providing some kind of statistics ^\r\n\r\nExample of output in Jenkins test plugin results, processing the XML file could be:\r\n```\r\nPackage                  Fail Skip Pass Total\r\n- total                     0    0   69    69\r\n- npm                       0    0   53    53\r\n  - mit                     0    0   45    45\r\n    - package1              0    0    1     1\r\n    - package 2             ...\r\n    - ...\r\n  - apache-2.0                              3\r\n    - package46\r\n    - ...\r\n  - bsd-3-clause                            1 \r\n    - package49\r\n  - isc                                     1\r\n    - package50\r\n  - unknown                                 3\r\n    - package51             1    0    0     1\r\n    - ...\r\n- go                                       16\r\n  - mit                                    14\r\n    - ...\r\n  - isc                                     1\r\n    - ...\r\n  - unknown                                 1\r\n    - packagexx             1    0    0     1\r\n    - ...\r\n```\r\nI did not fill the array; but you've got the point ;-)\r\n\r\nRegards,\r\n\r\nChris"</description>
</item>
<item>
	<title>Add a yarn dependency source</title>
	<link>https://github.com/github/licensed/issues/31</link>
<description>As found in https://github.com/github/licensed/issues/30#issuecomment-386129385\r\n\r\n> The errors appear to be because my package.json file is actually intended for yarn, and uses some syntax that is not npm-compatible\r\n\r\n[Yarn's](https://yarnpkg.com/) `package.json` files attempt to be compatible with NPM `package.json` files but can contain differences that cause the NPM dependency source to fail.\r\n\r\nIt looks like an indicator that yarn should be used and that npm shouldn't be used is the presence of a `yarn.lock` file."</description>
</item>
<item>
	<title>Turn parallel</title>
	<link>tests back on for CI</link>
<description>https://github.com/github/octocatalog-diff/issues/164</description>
</item>
<item>
	<title>How to use a different puppet configuration (e.g. strict</title>
	<link>variables)</link>
<description>https://github.com/github/octocatalog-diff/issues/158</description>
</item>
<item>
	<title>Invalid event parameters when setting a reminder</title>
	<link>https://github.com/github/opensourcefriday/issues/105</link>
<description>After signing up and clicking on Add to Calendar -> Google Calendar (under Set a reminder), getting the following error - \r\n![screen shot 2017-06-27 at 10 57 39 pm](https://user-images.githubusercontent.com/6823117/27601277-64e039ce-5b8c-11e7-91d0-6fc0170391ad.png)\r\n"</description>
</item>
<item>
	<title>delete-empty-repos.sh - Handle Pagination</title>
	<link>https://github.com/github/platform-samples/issues/190</link>
<description>[delete-empty-repos.sh](https://github.com/github/platform-samples/blob/master/api/bash/delete-empty-repos.sh) currently works with default pagination of only 30 repositories.\r\n\r\nNeed to handle pagination in case of more than 30 repositories are within the organization.\r\n\r\n[Relevant Doc](https://developer.github.com/v3/guides/traversing-with-pagination/)"</description>
</item>
<item>
	<title>Finish migrating to `pathtype` library.</title>
	<link>https://github.com/github/semantic/issues/288</link>
<description>The migration to `pathtype` has been overall a pleasant one—it makes functions much more indicative of their purpose, and it’s caught some bugs (though not serious ones). Right now we’re an uneasy hybrid of `pathtype` solutions and `FilePath`; we should move off of the latter completely. This involves overhauling `Semantic.CLI` to use `pathtype`; the [documentation](http://hackage.haskell.org/package/pathtype-0.8.1) provides a recipe to make `optparse-applicative` parsers do the right thing (respecting the path semantics; i.e., if we tell a parser to expect an absolute path, the parser will fail unless said path is actually absolute.)"</description>
</item>
<item>
	<title>Clojure(Script) support</title>
	<link>https://github.com/github/semantic/issues/217</link>
<description>Any possibility of adding Clojure/ClojureScript support?"</description>
</item>
<item>
	<title>Don't connect to kext when invoked with CLI args</title>
	<link>https://github.com/github/SoftU2F/issues/39</link>
<description>We try to connect to the kext when the app starts ([code](https://github.com/github/SoftU2F/blob/45825a5bd6e6bad7ae9bd681c8584cbcb6e7fb7b/SoftU2FTool/U2FHID.swift#L24)). This also happens when the app is launched from the command line (Eg. for deleting/listing registrations). If the app is already running in the background we'll get an error because the kext only allows one connection. This error is surfaced to the user, which is confusing. We should not connect to the kext unless we need to."</description>
</item>
<item>
	<title>Add Draft PR support</title>
	<link>https://github.com/github/VisualStudio/issues/2329</link>
<description>**Is your feature request related to a problem? Please describe.**\r\nI wouldn't say it's a problem, but it would be nice to add support to create draft PRs inside Visual Studio.\r\n\r\n**Describe the solution you'd like**\r\nDraft PRs will allow users to create PRs earlier in the development workflow and also provide more parity with dotcom.\r\n\r\nThis work would include being able to:\r\n- create draft PRs\r\n- view draft PRs\r\n- filter by a status of draft"</description>
</item>
<item>
	<title>elastic: write log entries to logs indices using a correct mapping for the @timestamp field</title>
	<link>https://github.com/grafana/fake-data-gen/issues/8</link>
<description>Currently data are written to logs indices, but I cannot use the indices as datasource in grafana because I got error that @timestamp is not a valie time field."</description>
</item>
<item>
	<title>Self signed certificates</title>
	<link>https://github.com/grafana/grafana-image-renderer/issues/33</link>
<description>I tested the render plugin against a Grafana instance with self signed certificate. I got the following error:\r\nError: net::ERR</description>
</item>
<item>
	<title>upgrade to schemaVersion 16</title>
	<link>https://github.com/grafana/grafonnet-lib/issues/66</link>
<description>Forgive my ignorance, but is this on the roadmap?  Is it worth doing today, or are we waiting for a subsequent/stable schema version to upgrade to?  I can potentially help, though I'd need a little bit of guidance.\r\n\r\nLooking at the [dashboard migration logic](https://github.com/grafana/grafana/blob/master/public/app/features/dashboard/dashboard</description>
</item>
<item>
	<title>Template current value does not work with multivalue template variables</title>
	<link>https://github.com/grafana/grafonnet-lib/issues/65</link>
<description>When trying to pass in multiple values for current value of template variable it does not generate json understood by grafana."</description>
</item>
<item>
	<title>docs: Labels recommendations</title>
	<link>https://github.com/grafana/loki/issues/1117</link>
<description>I was searching through our recommendations for labels and couldn't find anything.\r\n\r\nI think we should have in our user documentation some explanation about what are bad and good labels with some nice example.\r\n\r\nSomwhere here https://github.com/grafana/loki/blob/master/docs/clients/README.md\r\n\r\nWe should also explain how to have the same labels in Prometheus and Why ? (switching from metrics to logs).\r\n\r\n"</description>
</item>
<item>
	<title>Timestamp stage add a fudged timestamp for lines missing a timestamp</title>
	<link>https://github.com/grafana/loki/issues/1093</link>
<description>If using the timestamp stage and processing multi-line log lines, (or log lines that might not have a timestamp), the behavior could be a bit unpredictable.   \r\n\r\nIf the timestamp stage fails to match the timestamp, the promtail system time will be used for the log line, this creates a race between the last processed log lines timestamp and the system time, potentially leading to Loki rejecting the log line which had no timestamp as being older than the previous, or possibly the next log line with a timestamp being rejected if it's timestamp is older than what the system time applied was.\r\n\r\nI am suggesting an option be added to the timestamp stage to allow fudging a timestamp onto log lines which do not have one.  The stage should keep track of the previous lines timestamp and if the next line coming in doesn't have a timestamp, increment the previous timestamp by one nanosecond and use that as the timestamp.\r\n\r\nFudging a timestamp is not great and could potentially lead to problems however, with nanosecond precision I would hope this to be very unlikely.\r\n\r\nI also think the default for this behavior should be enabled, with an option to disable it"</description>
</item>
<item>
	<title>[helm] Deploying loki-stack with prometheus &amp; grafana fails in k8s 1.16</title>
	<link>https://github.com/grafana/loki/issues/1080</link>
<description>**Describe the bug**\r\nDue to deprecated `apiVersion`s removed in 1.16, the prometheus &amp; grafana dependencies fail to install.\r\n\r\n`stable/grafana` is fixed (currently 3.8.15) but `stable/prometheus` remains broken, pending helm/charts#17268.\r\n\r\n**To Reproduce**\r\n\r\n1. `helm install loki/loki-stack --set=grafana.enabled=true,prometheus.enabled=true`\r\n\r\n\r\n**Expected behavior**\r\nLoki, grafana, and prometheus installed.\r\n\r\n**Environment:**\r\n - Infrastructure: kubernetes 1.16\r\n - Deployment tool: helm\r\n\r\n**Screenshots, promtail config, or terminal output**\r\n\r\nAbove repro fails:\r\n> Error: apiVersion \"apps/v1beta2\" in loki-stack/charts/grafana/templates/deployment.yaml is not available\r\n\r\n\r\nFull errors (from `helm template | kubectl apply -f -`):\r\n```\r\nunable to recognize \"STDIN\": no matches for kind \"DaemonSet\" in version \"extensions/v1beta1\"\r\nunable to recognize \"STDIN\": no matches for kind \"Deployment\" in version \"apps/v1beta2\"\r\nunable to recognize \"STDIN\": no matches for kind \"Deployment\" in version \"extensions/v1beta1\"\r\nunable to recognize \"STDIN\": no matches for kind \"Deployment\" in version \"extensions/v1beta1\"\r\nunable to recognize \"STDIN\": no matches for kind \"Deployment\" in version \"extensions/v1beta1\"\r\nunable to recognize \"STDIN\": no matches for kind \"Deployment\" in version \"extensions/v1beta1\"\r\nunable to recognize \"STDIN\": no matches for kind \"PodSecurityPolicy\" in version \"extensions/v1beta1\"\r\nunable to recognize \"STDIN\": no matches for kind \"PodSecurityPolicy\" in version \"extensions/v1beta1\"\r\n```\r\n\r\nThe two policies are fixed by using `grafana==3.8.15`, but the remaining issues will require helm/charts#17268."</description>
</item>
<item>
	<title>Makefile doesn't yet include 1.14</title>
	<link>https://github.com/awslabs/amazon-eks-ami/issues/333</link>
<description>**What would you like to be added**:\r\nMakefile should include a target for 1.14\r\n\r\n**Why is this needed**:\r\n\r\nSo that I can run `make 1.14`. Support for 1.14 seems to otherwise be in the repo, just not in the Makefile"</description>
</item>
<item>
	<title>Deduplicate the command-line parameters</title>
	<link>https://github.com/awslabs/amazon-pollexy/issues/2</link>
<description>Several of the click options/arguments are shared across difference commands . . . need to de-dupe this code as much as possible."</description>
</item>
<item>
	<title>Add configuration of the config Aggregator as part of the deployment</title>
	<link>https://github.com/awslabs/aws-config-engine-for-compliance-as-code/issues/31</link>
<description>"</description>
</item>
<item>
	<title>New rule request: VPC</title>
	<link>SG</link>
<description>INBOUND</description>
</item>
<item>
	<title>New rule request: VPC</title>
	<link>AUTHORIZED</link>
<description>PEERING</description>
</item>
<item>
	<title>New rule request: VPC</title>
	<link>SG</link>
<description>OUTBOUND</description>
</item>
<item>
	<title>New Method in ConfigEvent</title>
	<link>https://github.com/awslabs/aws-config-rules/issues/143</link>
<description>The configEvent class from aws-lambda-java-events has been updated one of the methods and needs to be updated in the Source code also. \r\n isEventLeftScope -> getEventLeftScope "</description>
</item>
<item>
	<title>Add support for multiple deployoment OU in nested structure</title>
	<link>https://github.com/awslabs/aws-deployment-framework/issues/130</link>
<description>We're looking to implement ADF for our organisation, and at the moment it works pretty well for just our team. In future, if all goes smoothly with our spike, we'll likely be looking to roll this out to multiple teams in the larger group, we wouldn't want to share a deployment account since we need that separation (and there would be a lot of pipelines in there getting cluttered). However we do still share an Organization (billing reasons, Security, etc), and we think it would be great if ADF could support multiple deployment accounts in a similar structure as listed below:\r\n\r\n```\r\n+ root org\r\n     + team1\r\n          + deployment\r\n               - tooling-account (isolated to only allow deployment to units within team1)\r\n          + production\r\n               - team1-prod\r\n          + staging\r\n               - team1-staging\r\n     + team2\r\n          + deployment\r\n               - tooling-account (isolated to only allow deployment to units within team2)\r\n          + production\r\n               - team2-prod\r\n          + staging\r\n               - team2-staging\r\n     + team3\r\n          + deployment\r\n               - tooling-account (isolated to only allow deployment to units within team3)\r\n          + production\r\n               - team3-prod\r\n          + staging\r\n               - team3-staging\r\n```\r\n\r\nIs this the sort of feature that could be implemented at some point? Its my understanding that currently the tool only supports one deployment account due to the system-parameters upon initialisation.\r\n\r\nAnother structure that could work, but we aren't sure if its possible/how to implement:\r\n\r\n```\r\n+ root org\r\n    + deployment\r\n         - team1-tooling\r\n         - team2-tooling\r\n         - team3-tooling\r\n     + team1\r\n          + production\r\n               - team1-prod\r\n          + staging\r\n               - team1-staging\r\n     + team2\r\n          + production\r\n               - team2-prod\r\n          + staging\r\n               - team2-staging\r\n     + team3\r\n          + production\r\n               - team3-prod\r\n          + staging\r\n               - team3-staging\r\n```"</description>
</item>
<item>
	<title>Functions: Cannot specify 'Pinned' value</title>
	<link>https://github.com/awslabs/aws-greengrass-group-setup/issues/9</link>
<description>As the API is currently exposed, lambda functions referenced with gg</description>
</item>
<item>
	<title>Check for valid regions in GroupCommands</title>
	<link>https://github.com/awslabs/aws-greengrass-group-setup/issues/2</link>
<description>The `create`, `clean</description>
</item>
<item>
	<title>Feature Request: Auto-detect policy document fragment format</title>
	<link>https://github.com/awslabs/aws-iam-generator/issues/17</link>
<description>As a user, I should be able to write my IAM policy statements in either JSON or YAML and the tool should automatically handle either format.\r\n\r\nIdeally this would be a distinct feature from the `--format` flag introduced in https://github.com/awslabs/aws-iam-generator/pull/13 which IMO should be refactored to influence the CloudFormation output format only.\r\n"</description>
</item>
<item>
	<title>Create an operator that can detect scenes in video.</title>
	<link>https://github.com/awslabs/aws-media-insights-engine/issues/11</link>
<description>This operator will be very useful to help determine different scenes in video.  Applications include searching for ads, credits, slates, scenes.  Another application is that applying ML to unique scenes would save processing time by running ML on still frames, rather than on video."</description>
</item>
<item>
	<title>Add MediaTailor metrics to Monitor compartment</title>
	<link>https://github.com/awslabs/aws-media-services-application-mapper/issues/54</link>
<description>End-user would like to see current metrics for beacon nofilter, beacon ad only, beacon jingles only, if possible.\r\n"</description>
</item>
<item>
	<title>Add documentation for H/A configuration</title>
	<link>https://github.com/awslabs/aws-media-services-application-mapper/issues/9</link>
<description>Add HA</description>
</item>
<item>
	<title>Add documentation for MSAM REST API</title>
	<link>https://github.com/awslabs/aws-media-services-application-mapper/issues/8</link>
<description>REST</description>
</item>
<item>
	<title>Swagger UI demo</title>
	<link>https://github.com/awslabs/aws-serverless-java-container/issues/222</link>
<description>* Implementations: Jersey / Spring / Spring Boot / Spark\r\n\r\n## Scenario\r\nAdd the ability to serve a `swagger-ui.html` to the sample applications as mentioned in #214 \r\n\r\n## Expected behavior\r\nNavigating to `/ui/swagger-ui.html` should return the generated Swagger UI html page."</description>
</item>
<item>
	<title>Dropwizard example?</title>
	<link>https://github.com/awslabs/aws-serverless-java-container/issues/128</link>
<description>Can this framework support Dropwizard?  I have a REST server using dropwizard, JAX-RS, and hibernate and would like to port to Lambda.  If there are any examples of how to do this, please let me know.  Thanks"</description>
</item>
<item>
	<title>Apache Camel route is supported?</title>
	<link>https://github.com/awslabs/aws-serverless-java-container/issues/58</link>
<description>I was unable to run Apache Camel route in AWS Lambda. Do this container supports running Apache Camel route?"</description>
</item>
<item>
	<title>Support for Jooby framework</title>
	<link>https://github.com/awslabs/aws-serverless-java-container/issues/46</link>
<description>If it is convenient Support [Jooby ](http://jooby.org) framework?!"</description>
</item>
<item>
	<title>Add support for Play framework</title>
	<link>https://github.com/awslabs/aws-serverless-java-container/issues/19</link>
<description>Logging the feature request from our Twitter poll: https://twitter.com/sapessi/status/846816218422034432\r\n\r\nAdd support for the [Play Framework](https://www.playframework.com/)."</description>
</item>
<item>
	<title>trouble with connecting to dynamodb through Java</title>
	<link>https://github.com/awslabs/dynamodb-janusgraph-storage-backend/issues/233</link>
<description>Hi - I have the following Java code which I am trying to use to have Janusgraph connect to the cloud dynamodb (as opposed to the local test version).\r\n\r\nimport org.apache.commons.configuration.BaseConfiguration;\r\nimport com.amazon.janusgraph.diskstorage.dynamodb.DynamoDBStoreManager;\r\nimport org.janusgraph.core.JanusGraphFactory;\r\nimport org.janusgraph.core.JanusGraph;\r\n.....\r\nBaseConfiguration conf = new BaseConfiguration();\r\nconf.setProperty(\"storage.backend\",\"com.amazon.janusgraph.diskstorage.dynamodb.DynamoDBStoreManager\");\r\n....JanusGraph graph = JanusGraphFactory.open(conf);\r\n\r\nThe error I'm getting when trying to run is:\r\n\"errorMessage\": \"Could not instantiate implementation: com.amazon.janusgraph.diskstorage.dynamodb.DynamoDBStoreManager\",\r\n....\r\n\"errorType\": \"java.lang.NoClassDefFoundError\",\r\n\r\nIs this a bug or am I missing something? Thought importing com.amazon.janusgraph.diskstorage.dynamodb.DynamoDBStoreManager would be enough - I have dynamodb-janusgraph-storage-backend listed as a dependency in my pom.xml file so everything is compiling just fine.\r\n\r\nI've found examples of how to use Gremlin with dynamodb/janusgraph but nothing about how to set things up purely in Java. An example would be greatly appreciated."</description>
</item>
<item>
	<title>coio is not build in cargo.io migrate to tokyo the tls module</title>
	<link>https://github.com/awslabs/flowgger/issues/45</link>
<description>to unlock https://github.com/awslabs/flowgger/issues/32 coio has to go\r\n\r\nit's used in the TLS module as a scheduler and to control tcplistener and stream.\r\n\r\nThe package doesn't seem to have been touched for at least an year:\r\nhttps://github.com/zonyitoo/coio-rs\r\n\r\n"</description>
</item>
<item>
	<title>Enhance kafka commit</title>
	<link>https://github.com/awslabs/flowgger/issues/26</link>
<description>Hello,\r\n\r\nI think, it could be very useful to enhance \"Kafka output\" with a kind of timer option and associate it with the actual </description>
</item>
<item>
	<title>Question: netflow inputs?</title>
	<link>https://github.com/awslabs/flowgger/issues/14</link>
<description>Hi, would netflow/ipfix be an input of interest? "</description>
</item>
<item>
	<title>[question] any plans to be able to push directly to elasticsearch ?</title>
	<link>https://github.com/awslabs/flowgger/issues/11</link>
<description>"</description>
</item>
<item>
	<title>Metrics</title>
	<link>https://github.com/awslabs/flowgger/issues/1</link>
<description>Awesome code, thanks a lot.\n\nMay you please consider to expose some metrics?\nSuch as: uptime, number of active sessions, message received, sent, droped (charset or bad format).\n"</description>
</item>
<item>
	<title>Keras-MXNet RNN CPU performance is slower</title>
	<link>https://github.com/awslabs/keras-apache-mxnet/issues/99</link>
<description>Keras-MXNet is slower on CPU.\r\nSee early benchmark results here - https://github.com/awslabs/keras-apache-mxnet/blob/master/benchmark/benchmark</description>
</item>
<item>
	<title>Supporting MTCNN style model serving?</title>
	<link>https://github.com/awslabs/mxnet-model-server/issues/238</link>
<description>Hi there, \r\n\r\n    Great framework for serving mxnet based models!\r\n\r\n    I wonder if there is any suggestions on supporting multi-stage models such as MTCNN, https://github.com/pangyupo/mxnet</description>
</item>
<item>
	<title>Add heartbeat API to reference server</title>
	<link>https://github.com/awslabs/speke-reference-server/issues/13</link>
<description>https://docs.aws.amazon.com/speke/latest/documentation/heartbeat.html\r\n\r\nGET https://[server]/[stage]/heartbeat\r\n\r\n\r\n"</description>
</item>
<item>
	<title>Lambda zip for keyserver not updated during stack update</title>
	<link>https://github.com/awslabs/speke-reference-server/issues/10</link>
<description>Need to make CloudFormation aware that the keyserver zip has changed and requires an update. Right now CF doesn't know about a code change since only the zip contents change.\r\n"</description>
</item>
<item>
	<title>ClientLambdaFunction</title>
	<link>https://github.com/awslabs/speke-reference-server/issues/1</link>
<description>HI,\r\n\r\nI downloaded the SPEKE reference implementation and setup the server using the template provided. I can't seem to find the ClientLambdaFunction to set the following parameter:\r\n\r\n`CLIENT</description>
</item>

</channel>
</rss>
